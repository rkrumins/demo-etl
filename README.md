# Spark Data Lineage Framework

This framework provides a comprehensive solution for tracking data lineage across multiple Spark jobs. It's designed to capture the complete data flow from source files in HDFS through multiple transformation stages to the final data assets in Hive tables.

## Overview

The framework consists of three main components:

1. **Data Processing Job** - Reads raw data from HDFS files, applies transformations, and writes to an external Hive table
2. **Asset Creation Job** - Reads from the external table, applies further transformations, and creates a final analytics asset
3. **Workflow Driver Script** - Orchestrates the execution of both jobs and captures lineage information

## Data Flow and Lineage

The lineage is captured at multiple levels:

```
[HDFS Files] → [Job 1 Transformations] → [External Hive Table] → [Job 2 Transformations] → [Asset Hive Table]
```

### Lineage Tracking Methods

1. **Explicit Logging**: Both jobs log detailed lineage information at each transformation step
2. **Success Files**: The framework verifies completion using `_SUCCESS` files in HDFS
3. **Lineage Registry**: A dedicated Hive table stores lineage metadata
4. **Visualization**: A DOT graph file is generated to visualize the complete data flow

## Components

### 1. Data Processing Job (Job 1)

This Scala Spark job processes raw data from multiple HDFS files:

- **Inputs**: Three CSV files from HDFS (customers.csv, orders.csv, products.csv)
- **Transformations**: 
  - Filter active customers
  - Filter recent orders
  - Filter available products
  - Join data for sales analysis
  - Add derived columns
  - Calculate aggregations
  - Combine detailed and summary data
- **Output**: External Hive table (sales_data_external)

### 2. Asset Creation Job (Job 2)

This Scala Spark job creates analytics assets:

- **Input**: External Hive table from Job 1
- **Transformations**:
  - Extract customer analytics with enhanced metrics
  - Extract product analytics with ranking
  - Extract time-based analytics with trending
  - Create unified asset view across dimensions
- **Outputs**: 
  - Hive asset table (sales_analytics_asset)
  - Lineage registry entry

### 3. Workflow Driver Script

Bash script that:
- Orchestrates the execution of both Spark jobs
- Verifies input files and output tables
- Captures execution metadata
- Extracts lineage information from logs
- Generates a lineage visualization graph

## Key Features

1. **Complete Lineage Tracking**: Captures the full data journey from source to final asset
2. **Transformation Documentation**: Each transformation step is logged with inputs and outputs
3. **Multiple File Sources**: Demonstrates reading from multiple input files
4. **Complex Transformations**: Implements realistic data transformations in Scala
5. **Error Handling**: Includes validation checks throughout the workflow
6. **Visualization**: Provides a graphical representation of the data lineage

## Usage

1. Build the JAR files for both Spark jobs
2. Configure the paths in the workflow driver script
3. Run the workflow driver script:

```bash
./spark-workflow-driver.sh
```

## Lineage Visualization

The lineage graph generated by the workflow script can be rendered using Graphviz:

```bash
dot -Tpng -o lineage_graph.png /var/log/spark-lineage/lineage_graph_[WORKFLOW_ID].dot
```

## Requirements

- Apache Spark 3.x
- Hadoop/HDFS
- Hive
- Graphviz (for rendering lineage graphs)

## Best Practices Implemented

1. **Explicit Lineage Logging**: Detailed logging at each transformation step
2. **UUID for Jobs**: Unique identifiers for each workflow and job
3. **Timestamp Tracking**: Records start and end times for all operations
4. **Data Validation**: Verifies source and target data existence
5. **Metadata Capture**: Records file sizes and row counts
6. **Error Handling**: Graceful failure with meaningful error messages
7. **Visualization**: Graphical representation of lineage
8. **Documentation**: Complete documentation of all steps and processes

## Lineage Registry Schema

The lineage registry table maintains metadata about each job execution:

```
CREATE TABLE data_lineage_registry (
  job_id STRING,              -- Unique identifier for the job
  job_timestamp TIMESTAMP,    -- Execution timestamp
  source_path STRING,         -- Source data location
  target_path STRING,         -- Target data location
  transformation_count INT,   -- Number of transformations applied
  record_count BIGINT,        -- Number of records processed
  source_files STRING,        -- Source file names
  description STRING          -- Job description
)
```

This framework provides a complete solution for tracking data lineage across complex data pipelines, enabling data governance, audit trails, and impact analysis capabilities.
